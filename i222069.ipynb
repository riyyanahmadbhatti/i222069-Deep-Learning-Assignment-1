{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af699538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, cohen_kappa_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770302af",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./dataset\"   # change if needed\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "ANNOTATIONS_DIR = os.path.join(DATA_DIR, \"annotations\")\n",
    "OUTPUT_DIR = \"./outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "SEED = 42\n",
    "NUM_CLASSES = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "N_SAMPLES = 5496  # adjust if different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e6995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(ann_dir, img_dir, n_samples=N_SAMPLES):\n",
    "    records = []\n",
    "    for idx in range(n_samples):\n",
    "        exp_file = os.path.join(ann_dir, f\"{idx}_exp.npy\")\n",
    "        val_file = os.path.join(ann_dir, f\"{idx}_val.npy\")\n",
    "        aro_file = os.path.join(ann_dir, f\"{idx}_aro.npy\")\n",
    "        img_path = os.path.join(img_dir, f\"{idx}.jpg\")\n",
    "\n",
    "        # Skip if any required file missing\n",
    "        if not (os.path.exists(exp_file) and os.path.exists(val_file) and os.path.exists(aro_file) and os.path.exists(img_path)):\n",
    "            # optional: print(f\"Skipping index {idx} (missing file)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            expression = int(np.load(exp_file))\n",
    "            valence = float(np.load(val_file))\n",
    "            arousal = float(np.load(aro_file))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping index {idx}: load error -> {e}\")\n",
    "            continue\n",
    "\n",
    "        records.append({\n",
    "            \"filepath\": img_path,\n",
    "            \"expression\": expression,\n",
    "            \"valence\": valence,\n",
    "            \"arousal\": arousal\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "df = build_dataframe(ANNOTATIONS_DIR, IMAGES_DIR, n_samples=N_SAMPLES)\n",
    "print(\"Dataset size after filtering:\", len(df))\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_image_np(path, class_label, reg_label):\n",
    "    # Convert Tensor -> string\n",
    "    path = path.numpy().decode(\"utf-8\")\n",
    "    \n",
    "    # Open and preprocess image\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = img.resize(IMG_SIZE)\n",
    "    arr = np.asarray(img, dtype=np.float32) / 255.0\n",
    "    \n",
    "    # Labels\n",
    "    cls = np.int32(class_label.numpy())   # scalar int\n",
    "    reg = np.array(reg_label.numpy(), dtype=np.float32)\n",
    "    reg = reg.reshape(2)                  # enforce shape [2]\n",
    "    \n",
    "    return arr, cls, reg\n",
    "\n",
    "def tf_loader(path, label_class, label_reg):\n",
    "    arr, c, r = tf.py_function(\n",
    "        func=_load_image_np,\n",
    "        inp=[path, label_class, label_reg],\n",
    "        Tout=[tf.float32, tf.int32, tf.float32]\n",
    "    )\n",
    "    arr.set_shape([IMG_SIZE[0], IMG_SIZE[1], 3])\n",
    "    c.set_shape([])   # scalar\n",
    "    r.set_shape([2])  # vector length 2\n",
    "    return arr, {\"class_output\": c, \"reg_output\": r}\n",
    "\n",
    "def make_dataset(df, batch_size=32, shuffle=True):\n",
    "    paths = df[\"filepath\"].values.astype(str)\n",
    "    y_class = df[\"expression\"].values.astype(np.int32)\n",
    "    y_reg = df[[\"valence\", \"arousal\"]].values.astype(np.float32)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, y_class, y_reg))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(paths), seed=SEED)\n",
    "    ds = ds.map(tf_loader, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# quick test of dataset creation (run one batch)\n",
    "small_ds = make_dataset(df.head(8), batch_size=4, shuffle=False)\n",
    "for x, y in small_ds.take(1):\n",
    "    print(\"x shape:\", x.shape)\n",
    "    print(\"y class shape:\", y[\"class_output\"].shape)\n",
    "    print(\"y reg shape:\", y[\"reg_output\"].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_two_head_model(base_name=\"VGG16\", input_shape=(224,224,3), num_classes=NUM_CLASSES, lr=LEARNING_RATE):\n",
    "    if base_name == \"VGG16\":\n",
    "        base = VGG16(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    elif base_name == \"ResNet50\":\n",
    "        base = ResNet50(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported base model\")\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(base.output)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    # Classification head\n",
    "    c = layers.Dense(128, activation=\"relu\")(x)\n",
    "    c = layers.Dropout(0.3)(c)\n",
    "    c = layers.Dense(num_classes, activation=\"softmax\", name=\"class_output\")(c)\n",
    "\n",
    "    # Regression head\n",
    "    r = layers.Dense(64, activation=\"relu\")(x)\n",
    "    r = layers.Dropout(0.3)(r)\n",
    "    r = layers.Dense(2, activation=\"linear\", name=\"reg_output\")(r)\n",
    "\n",
    "    model = keras.Model(base.input, [c, r])\n",
    "\n",
    "    losses = {\"class_output\": \"sparse_categorical_crossentropy\", \"reg_output\": \"mse\"}\n",
    "    loss_weights = {\"class_output\": 1.0, \"reg_output\": 1.0}\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(LEARNING_RATE),\n",
    "                  loss=losses,\n",
    "                  loss_weights=loss_weights,\n",
    "                  metrics={\"class_output\": [\"accuracy\"]})\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_true, y_pred_proba):\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_bin = lb.fit_transform(y_true)\n",
    "    aucs, pr_aucs = [], []\n",
    "    # handle potential binary case\n",
    "    if y_true_bin.ndim == 1:\n",
    "        y_true_bin = y_true_bin.reshape(-1, 1)\n",
    "    for i in range(y_true_bin.shape[1]):\n",
    "        try:\n",
    "            precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "            pr_aucs.append(auc(recall, precision))\n",
    "            aucs.append(roc_auc_score(y_true_bin[:, i], y_pred_proba[:, i]))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1, \"kappa\": kappa,\n",
    "            \"auc\": float(np.mean(aucs)) if aucs else float('nan'),\n",
    "            \"pr_auc\": float(np.mean(pr_aucs)) if pr_aucs else float('nan')}\n",
    "\n",
    "def rmse(y_true, y_pred): return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "def corr(y_true, y_pred):\n",
    "    try:\n",
    "        return float(np.corrcoef(y_true.ravel(), y_pred.ravel())[0,1])\n",
    "    except Exception:\n",
    "        return float('nan')\n",
    "def sagr(y_true, y_pred): return float(np.mean(np.sign(y_true) == np.sign(y_pred)))\n",
    "def ccc(y_true, y_pred):\n",
    "    y_true, y_pred = y_true.ravel(), y_pred.ravel()\n",
    "    mean_true, mean_pred = np.mean(y_true), np.mean(y_pred)\n",
    "    var_true, var_pred = np.var(y_true), np.var(y_pred)\n",
    "    cov = np.mean((y_true-mean_true)*(y_pred-mean_pred))\n",
    "    denom = (var_true + var_pred + (mean_true-mean_pred)**2)\n",
    "    return float((2*cov) / denom) if denom != 0 else float('nan')\n",
    "\n",
    "def evaluate_regression(y_true, y_pred):\n",
    "    return {\n",
    "        \"rmse\": rmse(y_true, y_pred),\n",
    "        \"corr\": corr(y_true, y_pred),\n",
    "        \"sagr\": sagr(y_true, y_pred),\n",
    "        \"ccc\": ccc(y_true, y_pred)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(df, base=\"VGG16\", exp_name=\"exp\"):\n",
    "    # ensure enough samples and class distribution for stratify\n",
    "    if len(df) < 10:\n",
    "        raise ValueError(\"Not enough samples in df to train.\")\n",
    "    df_train, df_temp = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df[\"expression\"])\n",
    "    df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=SEED, stratify=df_temp[\"expression\"])\n",
    "\n",
    "    train_ds = make_dataset(df_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_ds = make_dataset(df_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_ds = make_dataset(df_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = build_two_head_model(base_name=base, num_classes=NUM_CLASSES, lr=LEARNING_RATE)\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n",
    "\n",
    "    # Predictions\n",
    "    y_true_cls, y_true_reg, y_pred_proba, y_pred_reg = [], [], [], []\n",
    "    for imgs, labels in test_ds:\n",
    "        cls_true = labels[\"class_output\"].numpy()\n",
    "        reg_true = labels[\"reg_output\"].numpy()\n",
    "        cls_pred, reg_pred = model.predict(imgs, verbose=0)\n",
    "        y_true_cls.extend(cls_true.tolist())\n",
    "        y_true_reg.extend(reg_true.tolist())\n",
    "        y_pred_proba.extend(cls_pred.tolist())\n",
    "        y_pred_reg.extend(reg_pred.tolist())\n",
    "\n",
    "    y_true_cls = np.array(y_true_cls)\n",
    "    y_true_reg = np.array(y_true_reg)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    y_pred_reg = np.array(y_pred_reg)\n",
    "\n",
    "    results = {\n",
    "        \"classification\": evaluate_classification(y_true_cls, y_pred_proba),\n",
    "        \"regression\": evaluate_regression(y_true_reg, y_pred_reg),\n",
    "        \"history\": history.history\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(OUTPUT_DIR, f\"{exp_name}_results.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    return results\n",
    "\n",
    "\n",
    "results_vgg = run_experiment(df, base=\"VGG16\", exp_name=\"VGG16\")\n",
    "results_resnet = run_experiment(df, base=\"ResNet50\", exp_name=\"ResNet50\")\n",
    "print(\"VGG16:\", results_vgg)\n",
    "print(\"ResNet50:\", results_resnet)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
